{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7a3f5f6c-2407-496b-b3c4-4f3012f52e81",
      "metadata": {
        "tags": [],
        "id": "7a3f5f6c-2407-496b-b3c4-4f3012f52e81",
        "outputId": "238e9852-e61e-4848-a61c-923b0e1830dc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: nltk in /home/ananthasubb/anaconda3/lib/python3.11/site-packages (3.8.1)\n",
            "Requirement already satisfied: click in /home/ananthasubb/anaconda3/lib/python3.11/site-packages (from nltk) (8.0.4)\n",
            "Requirement already satisfied: joblib in /home/ananthasubb/anaconda3/lib/python3.11/site-packages (from nltk) (1.2.0)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /home/ananthasubb/anaconda3/lib/python3.11/site-packages (from nltk) (2022.7.9)\n",
            "Requirement already satisfied: tqdm in /home/ananthasubb/anaconda3/lib/python3.11/site-packages (from nltk) (4.65.0)\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "pip install nltk"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "33bf6107-b745-4add-91cd-d4feabb4e7e3",
      "metadata": {
        "tags": [],
        "id": "33bf6107-b745-4add-91cd-d4feabb4e7e3",
        "outputId": "6c387004-8259-4087-efb8-c300a55109f8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pandas in /home/ananthasubb/anaconda3/lib/python3.11/site-packages (2.0.3)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /home/ananthasubb/anaconda3/lib/python3.11/site-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /home/ananthasubb/anaconda3/lib/python3.11/site-packages (from pandas) (2023.3.post1)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /home/ananthasubb/anaconda3/lib/python3.11/site-packages (from pandas) (2023.3)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /home/ananthasubb/anaconda3/lib/python3.11/site-packages (from pandas) (1.24.3)\n",
            "Requirement already satisfied: six>=1.5 in /home/ananthasubb/anaconda3/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "pip install pandas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "70a1b2b7-c68b-4563-977d-bd75f2084887",
      "metadata": {
        "tags": [],
        "id": "70a1b2b7-c68b-4563-977d-bd75f2084887",
        "outputId": "4881d630-453b-4e6e-a8a8-a34ebb0d0736"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                                               Token               Label\n",
            "0                                               Step                   O\n",
            "1                                                  1     B-EXAMPLE_LABEL\n",
            "2                                                  .                   O\n",
            "3                                         tert-Butyl  B-REACTION_PRODUCT\n",
            "4  ((3S,6R)-6-(fluoromethyl)tetrahydro-2H-pyran-3...  I-REACTION_PRODUCT\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Paths to your datasets\n",
        "train_data_path = 'combined_data_train.csv'\n",
        "test_data_path = 'combined_data_test.csv'\n",
        "dev_data_path = 'combined_data_dev.csv'\n",
        "\n",
        "# Load the datasets\n",
        "train_data = pd.read_csv(train_data_path)\n",
        "test_data = pd.read_csv(test_data_path)\n",
        "dev_data = pd.read_csv(dev_data_path)\n",
        "\n",
        "# Example of examining the first few rows of the training data\n",
        "print(train_data.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "56a97b1d-305d-40c0-b10b-ca4f6f433240",
      "metadata": {
        "id": "56a97b1d-305d-40c0-b10b-ca4f6f433240",
        "outputId": "2c43438e-cb8b-4cfc-e324-d1c5c2da00bc"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to\n",
            "[nltk_data]     /home/ananthasubb/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "from nltk.corpus import wordnet\n",
        "nltk.download('wordnet')\n",
        "\n",
        "def get_synonym(word):\n",
        "    synonyms = set()\n",
        "    for syn in wordnet.synsets(word):\n",
        "        for lemma in syn.lemmas():\n",
        "            synonyms.add(lemma.name())\n",
        "    if word in synonyms:\n",
        "        synonyms.remove(word)\n",
        "    return list(synonyms)[0] if synonyms else word"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b18ec182-bb8c-49b0-a5e5-4775cef86c78",
      "metadata": {
        "tags": [],
        "id": "b18ec182-bb8c-49b0-a5e5-4775cef86c78",
        "outputId": "687db67a-d6c0-4f1a-ff07-4c983cb36123"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2023-12-11 14:03:14.237632: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2023-12-11 14:03:14.237777: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2023-12-11 14:03:14.239302: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2023-12-11 14:03:14.248560: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2023-12-11 14:03:15.296728: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(2022, 128) (2022, 128, 25)\n",
            "(1102, 128) (1102, 128, 25)\n",
            "(200, 128) (200, 128, 25)\n"
          ]
        }
      ],
      "source": [
        "from transformers import AutoTokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "import numpy as np\n",
        "import random\n",
        "\n",
        "# Adjust the probability as needed\n",
        "SYNONYM_PROB = 0.36\n",
        "\n",
        "# Constants\n",
        "MAX_LEN = 128  # Adjust based on the length of your sentences\n",
        "BERT_MODEL_NAME = \"dmis-lab/biobert-base-cased-v1.1\"\n",
        "NUM_CLASSES = len(train_data['Label'].unique())  # Number of unique labels\n",
        "\n",
        "# Initialize the tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(BERT_MODEL_NAME)\n",
        "\n",
        "def concatenate_tokens_with_synonyms(data, synonym_prob=0.99):\n",
        "    sentences = []\n",
        "    labels = []\n",
        "    current_sentence = []\n",
        "    current_labels = []\n",
        "\n",
        "    for _, row in data.iterrows():\n",
        "        token = row['Token']\n",
        "        label = row['Label']\n",
        "\n",
        "        # Replace token with a synonym with a certain probability\n",
        "        if random.random() < synonym_prob:\n",
        "            token = get_synonym(token)\n",
        "\n",
        "        if token == \".\":\n",
        "            sentences.append(\" \".join(current_sentence))\n",
        "            labels.append(\" \".join(current_labels))\n",
        "            current_sentence = []\n",
        "            current_labels = []\n",
        "        else:\n",
        "            current_sentence.append(token)\n",
        "            current_labels.append(label)\n",
        "\n",
        "    return sentences, labels\n",
        "\n",
        "# Function to concatenate tokens into sentences\n",
        "def concatenate_tokens(data):\n",
        "    sentences = []\n",
        "    labels = []\n",
        "    current_sentence = []\n",
        "    current_labels = []\n",
        "\n",
        "    for _, row in data.iterrows():\n",
        "        if row['Token'] == \".\":\n",
        "            sentences.append(\" \".join(current_sentence))\n",
        "            labels.append(\" \".join(current_labels))\n",
        "            current_sentence = []\n",
        "            current_labels = []\n",
        "        else:\n",
        "            current_sentence.append(row['Token'])\n",
        "            current_labels.append(row['Label'])\n",
        "\n",
        "    return sentences, labels\n",
        "\n",
        "# Flag to control synonym replacement\n",
        "APPLY_SYNONYM_REPLACEMENT_TO_TEST_DEV = False  # Set to True to apply synonym replacement\n",
        "\n",
        "# For Training Data (Always apply synonym replacement)\n",
        "train_sentences, train_label_sentences = concatenate_tokens_with_synonyms(train_data, SYNONYM_PROB)\n",
        "\n",
        "# For Test Data\n",
        "if APPLY_SYNONYM_REPLACEMENT_TO_TEST_DEV:\n",
        "    test_sentences, test_label_sentences = concatenate_tokens_with_synonyms(test_data, SYNONYM_PROB)\n",
        "else:\n",
        "    test_sentences, test_label_sentences = concatenate_tokens(test_data)\n",
        "\n",
        "# For Dev Data\n",
        "if APPLY_SYNONYM_REPLACEMENT_TO_TEST_DEV:\n",
        "    dev_sentences, dev_label_sentences = concatenate_tokens_with_synonyms(dev_data, SYNONYM_PROB)\n",
        "else:\n",
        "    dev_sentences, dev_label_sentences = concatenate_tokens(dev_data)\n",
        "\n",
        "# Tokenization and padding\n",
        "def tokenize_and_pad(sentences):\n",
        "    input_ids = [tokenizer.encode(sentence, add_special_tokens=True) for sentence in sentences]\n",
        "    return pad_sequences(input_ids, maxlen=MAX_LEN, truncating='post', padding='post')\n",
        "\n",
        "train_inputs = tokenize_and_pad(train_sentences)\n",
        "test_inputs = tokenize_and_pad(test_sentences)\n",
        "dev_inputs = tokenize_and_pad(dev_sentences)\n",
        "\n",
        "# Label encoding and one-hot encoding\n",
        "label_encoder = LabelEncoder()\n",
        "label_encoder.fit(train_data['Label'])\n",
        "\n",
        "def encode_labels(label_sentences):\n",
        "    encoded_labels = [label_encoder.transform(label.split()) for label in label_sentences]\n",
        "    return pad_sequences(encoded_labels, maxlen=MAX_LEN, padding='post', value=label_encoder.transform(['O'])[0])\n",
        "\n",
        "train_labels = to_categorical(encode_labels(train_label_sentences), num_classes=NUM_CLASSES)\n",
        "test_labels = to_categorical(encode_labels(test_label_sentences), num_classes=NUM_CLASSES)\n",
        "dev_labels = to_categorical(encode_labels(dev_label_sentences), num_classes=NUM_CLASSES)\n",
        "\n",
        "# Checking shapes of the outputs\n",
        "print(train_inputs.shape, train_labels.shape)\n",
        "print(test_inputs.shape, test_labels.shape)\n",
        "print(dev_inputs.shape, dev_labels.shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9517565b-3556-4c91-b4a2-efce032f0c3e",
      "metadata": {
        "tags": [],
        "id": "9517565b-3556-4c91-b4a2-efce032f0c3e",
        "outputId": "eaa7f240-7e7e-409d-87c5-fa0ff5336cb5"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2023-12-11 14:03:29.940945: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1929] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 13775 MB memory:  -> device: 0, name: Tesla T4, pci bus id: 0000:27:00.0, compute capability: 7.5\n",
            "2023-12-11 14:03:30.485170: I external/local_tsl/tsl/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n",
            "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFBertModel: ['cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight']\n",
            "- This IS expected if you are initializing TFBertModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFBertModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "All the weights of TFBertModel were initialized from the PyTorch model.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                Output Shape                 Param #   Connected to                  \n",
            "==================================================================================================\n",
            " input_1 (InputLayer)        [(None, 128)]                0         []                            \n",
            "                                                                                                  \n",
            " input_2 (InputLayer)        [(None, 128)]                0         []                            \n",
            "                                                                                                  \n",
            " tf_bert_model (TFBertModel  TFBaseModelOutputWithPooli   1083102   ['input_1[0][0]',             \n",
            " )                           ngAndCrossAttentions(last_   72         'input_2[0][0]']             \n",
            "                             hidden_state=(None, 128, 7                                           \n",
            "                             68),                                                                 \n",
            "                              pooler_output=(None, 768)                                           \n",
            "                             , past_key_values=None, hi                                           \n",
            "                             dden_states=None, attentio                                           \n",
            "                             ns=None, cross_attentions=                                           \n",
            "                             None)                                                                \n",
            "                                                                                                  \n",
            " bidirectional (Bidirection  (None, 128, 100)             327600    ['tf_bert_model[0][0]']       \n",
            " al)                                                                                              \n",
            "                                                                                                  \n",
            " time_distributed (TimeDist  (None, 128, 25)              2525      ['bidirectional[0][0]']       \n",
            " ributed)                                                                                         \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 108640397 (414.43 MB)\n",
            "Trainable params: 108640397 (414.43 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "from transformers import TFBertModel\n",
        "from tensorflow.keras.layers import Input, Bidirectional, LSTM, Dense, TimeDistributed\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "# Constants\n",
        "MAX_LEN = 128  # Should be the same as used in preprocessing\n",
        "NUM_CLASSES = train_labels.shape[2]  # Based on the one-hot encoding\n",
        "\n",
        "# Load BERT model\n",
        "BERT_MODEL_NAME = \"dmis-lab/biobert-base-cased-v1.1\"\n",
        "bert_model = TFBertModel.from_pretrained(BERT_MODEL_NAME, from_pt=True)\n",
        "\n",
        "# Model architecture\n",
        "input_ids = Input(shape=(MAX_LEN,), dtype='int32')\n",
        "attention_mask = Input(shape=(MAX_LEN,), dtype='int32')\n",
        "bert_output = bert_model(input_ids, attention_mask=attention_mask)[0]\n",
        "bilstm = Bidirectional(LSTM(50, return_sequences=True))(bert_output)\n",
        "output = TimeDistributed(Dense(NUM_CLASSES, activation='softmax'))(bilstm)\n",
        "\n",
        "model = Model(inputs=[input_ids, attention_mask], outputs=output)\n",
        "model.compile(optimizer=Adam(learning_rate=3e-5), loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Summary of the model\n",
        "model.summary()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b10861ac-2189-45ec-b2da-48cb970a151f",
      "metadata": {
        "tags": [],
        "id": "b10861ac-2189-45ec-b2da-48cb970a151f",
        "outputId": "2c126ac2-2621-4a4a-c9ee-19bdc29eceea"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n",
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model/bert/pooler/dense/kernel:0', 'tf_bert_model/bert/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model/bert/pooler/dense/kernel:0', 'tf_bert_model/bert/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model/bert/pooler/dense/kernel:0', 'tf_bert_model/bert/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model/bert/pooler/dense/kernel:0', 'tf_bert_model/bert/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2023-12-11 14:03:59.961469: I external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:454] Loaded cuDNN version 8902\n",
            "2023-12-11 14:04:01.627456: I external/local_xla/xla/service/service.cc:168] XLA service 0x7fbae43e64c0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
            "2023-12-11 14:04:01.627588: I external/local_xla/xla/service/service.cc:176]   StreamExecutor device (0): Tesla T4, Compute Capability 7.5\n",
            "2023-12-11 14:04:01.638555: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "I0000 00:00:1702321441.872657 2228762 device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "39/64 [=================>............] - ETA: 21s - loss: 0.8149 - accuracy: 0.8642"
          ]
        }
      ],
      "source": [
        "# Training parameters\n",
        "EPOCHS = 50  # Adjust as needed\n",
        "BATCH_SIZE = 32  # Adjust as needed\n",
        "\n",
        "# Training the model\n",
        "history = model.fit(\n",
        "    [train_inputs, np.ones_like(train_inputs)],  # Assuming full attention masks\n",
        "    train_labels,\n",
        "    validation_data=([test_inputs, np.ones_like(test_inputs)], test_labels),\n",
        "    epochs=EPOCHS,\n",
        "    batch_size=BATCH_SIZE\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "22b689ad-cb1b-40be-b31b-021534b9788f",
      "metadata": {
        "tags": [],
        "id": "22b689ad-cb1b-40be-b31b-021534b9788f"
      },
      "outputs": [],
      "source": [
        "# Evaluate on the test set\n",
        "test_loss, test_accuracy = model.evaluate(\n",
        "    [dev_inputs, np.ones_like(dev_inputs)],  # Assuming full attention masks\n",
        "    dev_labels,\n",
        "    batch_size=BATCH_SIZE\n",
        ")\n",
        "\n",
        "print(f\"Test Loss: {test_loss}, Test Accuracy: {test_accuracy}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "95661647-2969-41b4-9053-ecc8e5063588",
      "metadata": {
        "tags": [],
        "id": "95661647-2969-41b4-9053-ecc8e5063588"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Plotting training and validation loss\n",
        "plt.figure(figsize=(12, 5))\n",
        "\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(history.history['loss'], label='Train Loss')\n",
        "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
        "plt.title('Training and Validation Loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "\n",
        "# Plotting training and validation accuracy\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(history.history['accuracy'], label='Train Accuracy')\n",
        "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
        "plt.title('Training and Validation Accuracy')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "12818a05-b301-46dd-a829-5817562fbd91",
      "metadata": {
        "tags": [],
        "id": "12818a05-b301-46dd-a829-5817562fbd91"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.utils import to_categorical\n",
        "from sklearn.metrics import classification_report\n",
        "import numpy as np\n",
        "\n",
        "# Predict labels on the test set\n",
        "test_predictions = model.predict([test_inputs, np.ones_like(test_inputs)])\n",
        "test_predictions = np.argmax(test_predictions, axis=-1)\n",
        "\n",
        "# Convert test labels from one-hot encoded to integer format\n",
        "true_test_labels = np.argmax(test_labels, axis=-1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "741d126e-a766-42b1-9967-6189856f489e",
      "metadata": {
        "tags": [],
        "id": "741d126e-a766-42b1-9967-6189856f489e"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import classification_report\n",
        "\n",
        "# Decode the predictions and true labels\n",
        "decoded_predictions = [label_encoder.inverse_transform(pred) for pred in test_predictions]\n",
        "decoded_true_labels = [label_encoder.inverse_transform(true_label) for true_label in true_test_labels]\n",
        "\n",
        "# Flatten the lists for classification report\n",
        "flat_predictions = [label for sublist in decoded_predictions for label in sublist]\n",
        "flat_true_labels = [label for sublist in decoded_true_labels for label in sublist]\n",
        "\n",
        "# Extract unique labels present in predictions and true labels\n",
        "unique_labels = sorted(set(flat_predictions + flat_true_labels))\n",
        "\n",
        "# Generate classification report\n",
        "report = classification_report(flat_true_labels, flat_predictions, target_names=unique_labels)\n",
        "\n",
        "print(report)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "eff8c36e-7dd5-4c73-b5d9-994fd4c961aa",
      "metadata": {
        "tags": [],
        "id": "eff8c36e-7dd5-4c73-b5d9-994fd4c961aa"
      },
      "outputs": [],
      "source": [
        "report = classification_report(flat_true_labels, flat_predictions, target_names=unique_labels, zero_division=1)\n",
        "print(report)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "312ec608-ea28-46b4-ac71-cb465d087466",
      "metadata": {
        "id": "312ec608-ea28-46b4-ac71-cb465d087466"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}